{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单轮对话人工评估对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切换工作目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Public/DialoGPT\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引用模组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import socket\n",
    "import subprocess as sp\n",
    "import sys\n",
    "from functools import partial\n",
    "from importlib import import_module\n",
    "from os.path import abspath, dirname, exists, join\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from demo_utils import download_model_folder\n",
    "from env import END_OF_TEXT_TOKEN\n",
    "from gpt2_training.train_utils import (boolean_string,\n",
    "                                       fix_state_dict_namespace,\n",
    "                                       get_eval_list_same_length, load_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义全局函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function of generate/filter/sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cut_seq_to_eos(sentence, eos_id, remove_id=[-1]):\n",
    "    sent = []\n",
    "    for s in sentence:\n",
    "        if s in remove_id:\n",
    "            continue\n",
    "        if s != eos_id:\n",
    "            sent.append(s)\n",
    "        else:\n",
    "            break\n",
    "    return sent\n",
    "\n",
    "\n",
    "# FROM HUGGING FACE REPO\n",
    "def top_filtering(logits, top_k=0, top_p=0.0, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
    "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
    "                whose total probability mass is greater than or equal to the threshold top_p.\n",
    "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
    "                the threshold top_p.\n",
    "            threshold: a minimal threshold to keep logits\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Back to unsorted indices and set them to -infinity\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    indices_to_remove = logits < threshold\n",
    "    logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def generate_next_token(model, input_ids, position_ids=None, token_type_ids=None, prev=None, temperature=1, top_k=0, top_p=0, past=None):\n",
    "    with torch.no_grad():\n",
    "        if not past:\n",
    "            hidden_states, past = model.transformer(prev, position_ids, token_type_ids, past=past)\n",
    "        else:\n",
    "            hidden_states, past = model.transformer(prev, past=past)\n",
    "        logits = model.lm_head(hidden_states)\n",
    "        logits = logits[0, -1, :] / temperature\n",
    "        logits = top_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "        probs = F.softmax(logits.unsqueeze(0), dim=-1)\n",
    "        prev = torch.multinomial(probs, num_samples=1)\n",
    "        return prev, probs[0][prev], past\n",
    "\n",
    "\n",
    "def generate_sequence(model, input_ids, position_ids=None, token_type_ids=None, temperature=1, top_k=0, top_p=0, length=20, past=None, device='cuda', eos_id=None):\n",
    "    output = input_ids.new_zeros([input_ids.size(0), 0])\n",
    "    prev = input_ids\n",
    "    for i in range(length):\n",
    "        prev, probs, past = generate_next_token(\n",
    "            model, input_ids, position_ids,\n",
    "            token_type_ids, prev, temperature, top_k, top_p, past\n",
    "        )\n",
    "        if eos_id is not None:\n",
    "            tokens = prev[0].cpu()\n",
    "            if tokens[0] == eos_id:\n",
    "                break\n",
    "        output = torch.cat((output, prev), dim=1)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function of load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(args):\n",
    "    tokenizer_class = args.tokenizer_class.strip()\n",
    "    module_name, class_name = tokenizer_class.split(':')\n",
    "    mod = import_module(module_name)\n",
    "    clz = getattr(mod, class_name)\n",
    "    return clz.from_pretrained(args.tokenizer_model)\n",
    "\n",
    "\n",
    "def load_tokenizer_and_model(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    args.device, args.n_gpu = device, n_gpu\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    torch.random.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    # load tokenizer\n",
    "    print('load tokenizer ...')\n",
    "    tokenizer = get_tokenizer(args)  # GPT2Tokenizer.from_pretrained(args.model_name_or_path)\n",
    "    eos_id = tokenizer.encode(END_OF_TEXT_TOKEN)[-1]\n",
    "    print('EOS: {:,d}'.format(eos_id))\n",
    "\n",
    "    # load the GPT-2 model\n",
    "    print('load the GPT-2 model ...')\n",
    "    config = GPT2Config.from_json_file(os.path.join(args.model_name_or_path, 'config.json'))\n",
    "    model = load_model(GPT2LMHeadModel(config), args.load_checkpoint, args, verbose=True)\n",
    "    print('load the GPT-2 model ok.')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return tokenizer, model, device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function of sigle turn dialog eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reply(tokenizer, model, device, history, args):\n",
    "    if isinstance(history, str):\n",
    "        history = [history]\n",
    "    assert all(isinstance(i, str) for i in history)\n",
    "    \n",
    "    eos_id = tokenizer.encode(END_OF_TEXT_TOKEN)[-1]\n",
    "    \n",
    "    context_tokens = sum([tokenizer.encode(h) + [eos_id] for h in history], [])  # + [eos_id]\n",
    "    context_tokens = torch.tensor(context_tokens, device=device, dtype=torch.long).unsqueeze(0)\n",
    "    position_ids = torch.arange(0, context_tokens.size(-1), dtype=torch.long, device=context_tokens.device)\n",
    "\n",
    "    out = generate_sequence(\n",
    "        model, context_tokens, position_ids=position_ids,\n",
    "        length=args.generation_length, temperature=args.temperature,\n",
    "        top_k=args.top_k, top_p=args.top_p,\n",
    "        eos_id=eos_id\n",
    "    )\n",
    "\n",
    "    out = out.tolist()\n",
    "    text = tokenizer.decode(out[0])\n",
    "\n",
    "    history.append(text)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数默认值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    seed=42,\n",
    "    tokenizer_class=None,\n",
    "    tokenizer_model=None,\n",
    "    model_name_or_path=None,\n",
    "    load_checkpoint=None,\n",
    "    fp16=True,\n",
    "    max_seq_length=1024,\n",
    "    generation_length=256,\n",
    "    temperature=0.7,\n",
    "    top_k=0,\n",
    "    top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_name_or_path = '/home/Public/DialoGPT/output_model/345m-hmwebmix-bpe-32k-v2/pre-train-345m-hmwebmix-bpe-32k-v2'\n",
    "args.load_checkpoint = '/home/Public/DialoGPT/output_model/345m-hmwebmix-bpe-32k-v2/GP2-pretrain-step-70000.pkl'\n",
    "\n",
    "args.tokenizer_class = 'tokenizers.tokenization_cn:GPT2BPETokenizer_CN'\n",
    "args.tokenizer_model = '/home/Public/DialoGPT/output_model/345m-hmwebmix-bpe-32k-v2/pre-train-345m-hmwebmix-bpe-32k-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tokenizer ...\n",
      "EOS: 6\n",
      "load the GPT-2 model ...\n",
      "load the GPT-2 model ok.\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model, device = load_tokenizer_and_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从测试集进行单轮对话。\n",
    "\n",
    "数据格式:\n",
    "\n",
    "- tsv\n",
    "- 两列\n",
    "  - 第一列: A 的提问\n",
    "  - 第二列: B 的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9108 /home/Public/data/transfer-learning/output/output-qa/xinli001_jiandanxinli-qa.topics_去重_DialogGPT/test_0_DialogGPT.tsv\n"
     ]
    }
   ],
   "source": [
    "test_data_file = '/home/Public/data/transfer-learning/output/output-qa/xinli001_jiandanxinli-qa.topics_去重_DialogGPT/test_0_DialogGPT.tsv'\n",
    "\n",
    "!wc -l {test_data_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e5c1c9668248ba9819a1949931d926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total = 500\n",
    "\n",
    "with open(test_data_file) as csvfile, \\\n",
    "     open('out/test-result.70k.json', 'w') as fp:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t')\n",
    "    for _, row in tqdm(zip(range(total), reader), total=500):\n",
    "        history = row[0]\n",
    "        histsory = generate_reply(tokenizer, model, device, history, args)\n",
    "        reply = histsory[-1]\n",
    "        data = {\n",
    "            'question': row[0],\n",
    "            'generate': reply,\n",
    "            'answer': row[1],\n",
    "        }\n",
    "        print(json.dumps(data, ensure_ascii=False), file=fp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('DialoGPT-dev': conda)",
   "language": "python",
   "name": "python37664bitdialogptdevconda85c8c7bfcca841bf9d684a0dd06d4435"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
